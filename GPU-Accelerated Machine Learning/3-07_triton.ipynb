{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453524a2-0d79-4512-8d97-0240ec1ea08e",
   "metadata": {},
   "source": [
    "<img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b540f-18e8-42c3-a688-c30103c5f462",
   "metadata": {},
   "source": [
    "# Accelerating End-to-End Data Science Workflows # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa05f5-febf-487c-bd59-d489efd98709",
   "metadata": {},
   "source": [
    "## 07 - Triton ##\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the process of deploying a model with Triton Inference Server and walks through the various tools available within Triton. This notebook covers the below sections: \n",
    "1. [Background](#s1)\n",
    "2. [Preparing the Model](#s1)\n",
    "    * [Load the Model](#s1)\n",
    "    * [Creating the Folder Structure](#s1)\n",
    "    * [Creating the Configuration File](#s1)\n",
    "3. [Loading the Model in Triton](#s1)\n",
    "    * [Starting Triton](#s1)\n",
    "    * [Check Status of Triton Server](#s1)\n",
    "4. [Testing Inference](#s1)\n",
    "    * [Triton Client](#s1)\n",
    "    * [Verifying Results for Local Model and Triton](#s1)\n",
    "5. [Analyzing Performance](#s1)\n",
    "    * [Customizing Perf Analyzer](#s1)\n",
    "    * [Exercise #1 - Testing Perf Analyzer](#s1)\n",
    "6. [Model Analyzer](#s1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d08487-8161-4d8c-b402-cdeab3fcbe2a",
   "metadata": {},
   "source": [
    "# Background\n",
    "NVIDIA offers a framework for deploying ML models called Triton. Triton will automatically handle all inference requests that comes to the server. Triton supports multiple backends including PyTorch, TensorFlow, Forest Inference Library (FIL), etc. In this notebook, we will focus on the FIL backend using the xgboost model that was trained previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a7d94-c0a9-4b86-92bb-fafffa1ed021",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "### Load the Model\n",
    "Let's start with loading the previous XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2149c-3f83-46cd-a1eb-70db7067cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.Booster({'nthread': 4})  # init model\n",
    "model.load_model('xgboost_model.json')  # load model data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7896106-8410-40c3-8b6d-e0bdb94f69c3",
   "metadata": {},
   "source": [
    "### Creating the Folder Structure\n",
    "In the previous notebook, we just saved the XGBoost model to the working directory. Triton expects the model to be in a particular structure: Model name should be the top level directory, and the version number should be next. This allows for multiple models and versions of those models to be hosted simulatenously. Let's create that folder structure and save the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c42b97-2691-46d1-9b3a-7d19685d1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('models')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)\n",
    "\n",
    "# The name of the model directory determines the name of the model as reported by Triton\n",
    "model_dir = os.path.join(REPO_PATH, \"virus_prediction\")\n",
    "\n",
    "# We can store multiple versions of the model in the same directory. In our case, we have just one version, so we will add a single directory, named '1'.\n",
    "version_dir = os.path.join(model_dir, '1')\n",
    "os.makedirs(version_dir, exist_ok=True)\n",
    "\n",
    "# The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "# It is recommended that you use this filename to avoid having to specify a name in the configuration file.\n",
    "model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "model.save_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e5837-4807-4fd4-af21-0973a4942f1b",
   "metadata": {},
   "source": [
    "### Creating the Configuration File\n",
    "Triton also requires a configuration file that provides details about the model and deployment. For this notebook, we will assume default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d898fb-a8d2-4d1c-a13f-2c4be6c18969",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {32768}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 4 ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: KIND_GPU }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"false\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262d4d9-819a-4d28-bb26-7eb737c3387a",
   "metadata": {},
   "source": [
    "Now the model is ready to be loaded in Triton! The model repository should look like this.\n",
    "\n",
    "```\n",
    "model/\n",
    "`-- virus_prediction\n",
    "    |-- 1\n",
    "    |   `-- xgboost.model\n",
    "    `-- config.pbtxt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5876d7a-3416-4ac9-827a-53796a1f7f93",
   "metadata": {},
   "source": [
    "## Loading the Model in Triton\n",
    "Next, we will need to start the Triton server. For this course, the server has already been started, but we will briefly go the steps required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cf499-9faa-4746-9252-16f41fa548e6",
   "metadata": {},
   "source": [
    "### Starting Triton\n",
    "Triton is available as buildable source code or a pre-built docker image. For simplicity, we recommend that most users start with the docker images. This is how you would start the docker container in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283515c-c812-41c5-a100-e4dc5788781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker run --gpus=1 --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded6e36-0a1d-488e-8b10-f482ced8ea07",
   "metadata": {},
   "source": [
    "Wow! That's a lot of inputs. Let's break it down a little bit more.\n",
    "\n",
    "- **gpus=1** : Passes through the first GPU to the Triton Inference Server\n",
    "- **rm** : Removes the container after completing execution\n",
    "- **p 8000:8000** : Forwards the GTPCInferenceService Port\n",
    "- **p 8001:8001** : Forwards the HTTPService Port\n",
    "- **p 8002:8002** :  Forwards the Metrics Port\n",
    "- **v /full/path/to/docs/examples/model_repository:/models** : Mounts the path to the models folder on the host machine to the Triton Inference Server container\n",
    "- **nvcr.io/nvidia/tritonserver:<xx.yy>-py3** : Name of the Triton Inference Server image. The version number will change depending on the latest release\n",
    "- **tritonserver --model-repository=/models**: The command to run in the container. In this case, we start the Triton Inference Server and point to the models folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42967a82-fdfd-413e-8006-eb78a3ca805b",
   "metadata": {},
   "source": [
    "As we mentioned before, the server has already been started for this lab. Let's check our connection to the server! We are using \"triton\" as the hostname because the default docker network will resolve \"triton\" to the ip address of the Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac240cdc-f0dc-45ad-9838-58066a85696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b9b35-51c6-4eba-a9c0-9a92d19e0a0b",
   "metadata": {},
   "source": [
    "Now let's see if the model has been loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b37c5-1242-489f-8463-387e657264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://triton:8000/v2/repository/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06902d34-7763-4693-b919-f70ee4d60dfa",
   "metadata": {},
   "source": [
    "If all went well, we should be able to see the model \"virus prediction\" show up as ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429b1bf-9372-42a5-86ea-be07cabf30ed",
   "metadata": {},
   "source": [
    "## Testing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2616f6-2de8-4e84-96d1-ad92b5f4b732",
   "metadata": {},
   "source": [
    "### Triton Client\n",
    "To test the deployment we will use the Triton Client library. Let's see how we can create an instance of the client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec652ce4-a2d6-4b3a-8e2f-faac51da5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = \"triton\"\n",
    "PORT = 8001\n",
    "TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af3af5-1d15-406f-91cb-2208c938d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3a594-e377-47c9-b3e7-0da321138bf5",
   "metadata": {},
   "source": [
    "Now let's make sure the Triton server is ready by sending a sample inference request. First let's load the training data. We will only load 32768 rows, as that's the max batch size we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57396e8-bc06-43dd-bf4c-2e34b24f5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf \n",
    "import numpy as np\n",
    "df = cudf.read_csv('./data/clean_uk_pop_full.csv', usecols=['age', 'sex', 'northing', 'easting', 'infected'], nrows=5000000)\n",
    "df = df.sample(32768)\n",
    "input_data = df.drop('infected', axis=1)\n",
    "target = df[['infected']]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f3792-a541-4b9f-acdc-e72c64897804",
   "metadata": {},
   "source": [
    "Now we convert it to a numpy array and force the type to be float32 (the same type we specified in the config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab43811-f273-48cf-b473-ea3d4786cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df = input_data.to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49f912-5e35-4fe8-b6fa-b91f025cc07e",
   "metadata": {},
   "source": [
    "Since we limited our batch size to 32768, let's splice the array and attempt the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e90afc-9ea6-45ad-83bb-2ac7f154b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batched_data = converted_df[:32768]\n",
    "# Prepare the input tensor\n",
    "input_tensor = triton_grpc.InferInput(\"input__0\", batched_data.shape, 'FP32')\n",
    "input_tensor.set_data_from_numpy(batched_data)\n",
    "\n",
    "# Prepare the output\n",
    "output = triton_grpc.InferRequestedOutput(\"output__0\")\n",
    "\n",
    "# Send inference request\n",
    "response = client.infer(\"virus_prediction\", [input_tensor], outputs=[output])\n",
    "\n",
    "# Get the output data\n",
    "output_data = response.as_numpy(\"output__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c2923-acc1-48cb-bc41-3adafbf7eca2",
   "metadata": {},
   "source": [
    "Let's make sure the results we got were the same as with the local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809c2db-11f9-48de-9913-3e032e14a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "xgb_data = xgb.DMatrix(input_data)\n",
    "y_test = model.predict(xgb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ada8b8-0f47-435b-90df-57c5056bfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we got the same accuracy as previously\n",
    "#target = target.to_numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(target.to_numpy(), y_test)\n",
    "auc_result = auc(false_pos_rate, true_pos_rate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(false_pos_rate, true_pos_rate, lw=3,\n",
    "        label='AUC = {:.2f}'.format(auc_result))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend(loc='lower right');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30562baa-f7c9-40d8-ab5d-40b3c642b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we got the same accuracy as previously\n",
    "#target = target.to_numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "false_pos_rate, true_pos_rate, thresholds = roc_curve(target.to_numpy(), output_data)\n",
    "auc_result = auc(false_pos_rate, true_pos_rate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(false_pos_rate, true_pos_rate, lw=3,\n",
    "        label='AUC = {:.2f}'.format(auc_result))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend(loc='lower right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00984cb-6e04-4951-a864-a67cc4a22b6f",
   "metadata": {},
   "source": [
    "As we can see, our AUC score is the same between both inference options!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459c429-b902-4056-ad2e-05d53f82537c",
   "metadata": {},
   "source": [
    "## Analyzing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724bd09-057b-408a-896a-b48a7a4362d1",
   "metadata": {},
   "source": [
    "Earlier, we tested a *relatively* small inference request. What if we want to see the max throughput of the model? Luckily, Triton offers a performance analysis tool that generates synthetic data to collect latency and throughput numbers. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15129aef-3d3f-4c65-b2ff-1d0b2599ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m virus_prediction -u \"triton:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe9a4c-e5db-4474-bc9f-76e0f3485506",
   "metadata": {},
   "source": [
    "That's a lot of information to take in. Let's break it down.\n",
    "\n",
    "**Measurement window**: Timeframe that measurements are taken in \n",
    "\n",
    "**Batch Size**: Number of inputs in each request\n",
    "\n",
    "\n",
    "**Concurrency**: Number of simulatenous connections\n",
    "**Latency**: Time taken to recieve results\n",
    "**p50/90/95/99**: Different percentiles for latency\n",
    "\n",
    "Based on these results, we can see that our throughput is roughly ~2300 inferences per second with a single concurrent connection, and the average latency for each requst is 434 usec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8c5c0-7f3e-41e3-881d-38f9d0aa5d3e",
   "metadata": {},
   "source": [
    "# Customizing Perf Analyzer\n",
    "The Performance Analyzer tool for Triton has many knobs that can be turned to analyze results. Let's enable GPU metric collection, and increase the batch size and concurrency range values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353c5bb-ebd5-492e-ad62-90e8929849d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer --collect-metrics -m virus_prediction -u \"triton:8000\" -b 8 --concurrency-range 2:8:2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0340a9-da5a-400f-9127-b3f4fe1facad",
   "metadata": {},
   "source": [
    "The results show that our model configuration gives a throughput of about ~156171 inferences per second. Notice how there are significant throughput gains as we increase the number of concurrent connections. With low concurrency values, Triton is idle during the time when the response is returned to the client and the next request is received at the server. Throughput increases when we increase concurrency values because Triton overlaps the processing of one request with the communication of the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13ffda-b03d-435e-a01e-5f8ceaa076de",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "Please take this time to experiment with the perf analyzer tool. A full list of parameters can be seen with the --help argument. Some parameters that we recommend trying out are listed below. \n",
    "\n",
    "-b <value> : batch size\n",
    "--concurrency-range <start:end:step> : range of concurrency values to test\n",
    "--collect-metrics : enable collection of GPU metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb02a7-92c1-4373-afb6-c4a507ddf20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ccd1e7-989b-49b5-8ece-0583e9890c66",
   "metadata": {},
   "source": [
    "## Model Analyzer\n",
    "Although it is out of scope for this course, we would like to introduce the Model Analyzer tool that is available as part of Triton. This tool searches through different parameter configurations to find optimal parameters that maximize inference throughput. With some minor processing, results can be viewed in a PDF format as well. The syntax for running Model Analyzer is shown below, as well as an example of the output. Model Analyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a896c-8f30-423d-baa9-db85fb66ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "## Writing constraints to file\n",
    "#cat > model_analyzer_constraints.yaml <<EOL \n",
    "#model_repository: /model_repository/\n",
    "#triton_launch_mode: \"local\"\n",
    "#latency_budget: 5\n",
    "#run_config_search_max_concurrency: 64\n",
    "#run_config_search_max_instance_count: 3\n",
    "#run_config_search_max_preferred_batch_size: 8\n",
    "#profile_models:\n",
    "#  virus_prediction\n",
    "#\n",
    "#EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3598a-a61f-4bcc-a079-d43e57d5d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model_analyzer profiler on XGBoost model \n",
    "#!model-analyzer profile -f model_analyzer_constraints.yaml --override-output-model-repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0c90f-d8e6-4ce5-a9c9-197a46e1d8a2",
   "metadata": {},
   "source": [
    "![image](images/model_analyzer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb441f66-4e12-44b2-b940-2fecef8d078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8780e-671f-46e7-8754-5005fd61ae7d",
   "metadata": {},
   "source": [
    "**Well Done!** Let's move to the [next notebook](3-08_k-means_dask.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ce5b2-ec8f-414e-acc7-63702cf1b7a7",
   "metadata": {},
   "source": [
    "<img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
